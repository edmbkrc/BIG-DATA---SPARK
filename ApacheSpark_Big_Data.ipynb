{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bAGWcTY5u3xG",
    "outputId": "68645b71-2f86-4693-b611-5be795346cd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fampr4pGvAM_",
    "outputId": "ecd4bddb-3cf8-4bf5-c71d-5ebdf33bfc1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 16, 3136, 49, 16, 4]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "sc = SparkContext()\n",
    "rdd = sc.parallelize([3,4,56,7,4,2])\n",
    "\n",
    "sq = rdd.map(lambda x: x*x)\n",
    "print(sq.collect())\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njk9dBj4y5xF"
   },
   "source": [
    "## Ratings Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQeJMBA1vRam",
    "outputId": "a80f073f-60ae-42b0-d9c0-ae560e3c0fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6111\n",
      "2 11370\n",
      "3 27145\n",
      "4 34174\n",
      "5 21203\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "conf=SparkConf().setMaster('local').setAppName(\"RatingsHistogram\")\n",
    "sc=SparkContext(conf=conf)\n",
    "\n",
    "lines=sc.textFile('u.data')\n",
    "ratings=lines.map(lambda x:x.split()[2])\n",
    "result=ratings.countByValue()\n",
    "sortedResults=collections.OrderedDict(sorted(result.items()))\n",
    "for key,value in sortedResults.items():\n",
    "  print(key,value)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EYXML4qrzrHP"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1JUNfTQo2ojY",
    "outputId": "12550152-ad4d-40f8-ce3f-87119ef50615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITE00100554\t5.36F\n",
      "EZE00100082\t7.70F\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MinTemperatures\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "lines = sc.textFile(\"1800.csv\")\n",
    "parsedLines = lines.map(parseLine)\n",
    "minTemps = parsedLines.filter(lambda x: \"TMIN\" in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))\n",
    "results = minTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "99j3O2Vy7T4u"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJWpCUmr7ntU",
    "outputId": "58f2da20-ec53-4636-81d5-9c672af135f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITE00100554\t90.14F\n",
      "EZE00100082\t90.14F\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MaxTemperatures\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "lines = sc.textFile(\"1800.csv\")\n",
    "parsedLines = lines.map(parseLine)\n",
    "minTemps = parsedLines.filter(lambda x: \"TMAX\" in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: max(x,y))\n",
    "results = minTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vol_2jq68osF"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"BookWordCount\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "input = sc.textFile(\"book.txt\")\n",
    "words = input.flatMap(lambda x: x.split())\n",
    "wordCounts = words. countByValue()\n",
    "\n",
    "for word, count in wordCounts.items():\n",
    "  print(word,count)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MYTK2-rcDRMW"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09l7Y701Dw96"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def normalizeWords(text):\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "input = sc.textFile(\"book.txt\")\n",
    "words = input.flatMap(normalizeWords)\n",
    "\n",
    "wordCounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "wordCountsSorted = wordCounts.map(lambda x: (x[1], x[0])).sortByKey(False)\n",
    "results = wordCountsSorted.collect()\n",
    "\n",
    "for result in results:\n",
    "    count = str(result[0])\n",
    "    word = result[1].encode('ascii', 'ignore')\n",
    "    if (word):\n",
    "        print(word.decode() + \":\\t\\t\" + count)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CFaTY7OEcFK",
    "outputId": "d0e68857-5c32-4d67-f84d-3fbffae1f3de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'CAPTAIN AMERICA' is the most popular superhero, with 1933 co-appearances.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"PopularHero\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def countCoOccurences(line):\n",
    "    elements = line.split()\n",
    "    return (int(elements[0]), len(elements) - 1)\n",
    "\n",
    "def parseNames(line):\n",
    "    fields = line.split('\\\"')\n",
    "    return (int(fields[0]), fields[1].encode(\"utf8\"))\n",
    "\n",
    "names = sc.textFile(\"Marvel-names.txt\")\n",
    "namesRdd = names.map(parseNames)\n",
    "\n",
    "lines = sc.textFile(\"Marvel-graph.txt\")\n",
    "\n",
    "pairings = lines.map(countCoOccurences)\n",
    "totalFriendsByCharacter = pairings.reduceByKey(lambda x, y : x + y)\n",
    "flipped = totalFriendsByCharacter.map(lambda xy : (xy[1], xy[0]))\n",
    "\n",
    "mostPopular = flipped.max()\n",
    "\n",
    "mostPopularName = namesRdd.lookup(mostPopular[1])[0]\n",
    "\n",
    "print(str(mostPopularName) + \" is the most popular superhero, with \" + str(mostPopular[0]) + \\\n",
    "      \" co-appearances.\")\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L7wVLso6G0y_",
    "outputId": "fb0aa9f7-b93a-4157-9dc7-6f9ed05e1a61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8546265328874024\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Step 1: Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"PimaIndianClassification\").getOrCreate()\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "data = spark.read.csv(\"pima-indians-diabetes.csv\", inferSchema=True, header=True)\n",
    "\n",
    "# Step 3: Prepare the data for training\n",
    "feature_columns = data.columns[:-1]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(data).select(\"features\", \"Outcome\")\n",
    "\n",
    "# Step 4: Split the data into training and testing sets\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Step 5: Train a logistic regression model\n",
    "lr = LogisticRegression(labelCol=\"Outcome\", featuresCol=\"features\")\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Step 6: Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Outcome\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vw93LRNtJhI3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
